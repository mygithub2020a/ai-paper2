{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Belavkin Optimizer Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze results from Belavkin optimizer experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from belavkin_ml.utils.visualization import (\n",
    "    plot_training_curves,\n",
    "    plot_optimizer_comparison,\n",
    "    plot_convergence_analysis,\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load experimental results from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from modular arithmetic experiment\n",
    "results_path = Path('../experiments/track1/results/modular_arithmetic/results.json')\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Loaded results for: {list(results.keys())}\")\n",
    "else:\n",
    "    print(f\"Results not found. Run experiments first.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Plot comparison\n",
    "    fig = plot_optimizer_comparison(\n",
    "        all_results=results,\n",
    "        metric='test_accs',\n",
    "        title='Test Accuracy - Modular Arithmetic',\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    fig = plot_convergence_analysis(all_results=results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Compute statistics for each optimizer\n",
    "    stats = {}\n",
    "    \n",
    "    for opt_name, opt_results in results.items():\n",
    "        final_accs = [r['final_test_acc'] for r in opt_results]\n",
    "        conv_epochs = [r['convergence_epoch'] for r in opt_results \n",
    "                       if r['convergence_epoch'] is not None]\n",
    "        \n",
    "        stats[opt_name] = {\n",
    "            'final_acc_mean': np.mean(final_accs),\n",
    "            'final_acc_std': np.std(final_accs),\n",
    "            'conv_epoch_mean': np.mean(conv_epochs) if len(conv_epochs) > 0 else None,\n",
    "            'conv_epoch_std': np.std(conv_epochs) if len(conv_epochs) > 0 else None,\n",
    "            'conv_rate': len(conv_epochs) / len(opt_results),\n",
    "        }\n",
    "    \n",
    "    # Display as table\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(stats).T\n",
    "    print(\"\\nOptimizer Statistics:\")\n",
    "    print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learning dynamics\n",
    "if results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    for opt_name, opt_results in results.items():\n",
    "        epochs = opt_results[0]['epochs']\n",
    "        \n",
    "        # Average over seeds\n",
    "        train_losses_all = [r['train_losses'] for r in opt_results]\n",
    "        mean_loss = np.mean(train_losses_all, axis=0)\n",
    "        \n",
    "        axes[0].plot(epochs, mean_loss, label=opt_name, linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Train-test gap\n",
    "    for opt_name, opt_results in results.items():\n",
    "        epochs = opt_results[0]['epochs']\n",
    "        \n",
    "        train_accs = np.mean([r['train_accs'] for r in opt_results], axis=0)\n",
    "        test_accs = np.mean([r['test_accs'] for r in opt_results], axis=0)\n",
    "        \n",
    "        gap = train_accs - test_accs\n",
    "        \n",
    "        axes[1].plot(epochs, gap, label=opt_name, linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Train-Test Gap')\n",
    "    axes[1].set_title('Generalization Gap')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Paper\n",
    "\n",
    "Generate publication-quality figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set publication style\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "\n",
    "# Generate figure\n",
    "if results:\n",
    "    fig = plot_optimizer_comparison(\n",
    "        all_results=results,\n",
    "        metric='test_accs',\n",
    "        title='',  # Remove title for paper\n",
    "    )\n",
    "    \n",
    "    # Save as PDF for LaTeX\n",
    "    fig.savefig('../papers/track1_optimizer/figures/main_result.pdf', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved figure to papers/track1_optimizer/figures/main_result.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
