{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Belavkin Optimizer\n",
    "\n",
    "This notebook demonstrates the basic usage of the Belavkin optimizer on a simple modular arithmetic task.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Belavkin optimizer implements a novel update rule inspired by quantum filtering:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - [\\gamma(\\nabla L(\\theta))^2 + \\eta\\nabla L(\\theta)]\\Delta t + \\beta\\nabla L(\\theta)\\sqrt{\\Delta t}\\epsilon$$\n",
    "\n",
    "where:\n",
    "- $\\gamma$: Damping factor (measurement backaction)\n",
    "- $\\eta$: Learning rate\n",
    "- $\\beta$: Exploration factor\n",
    "- $\\epsilon \\sim N(0,1)$: Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from belavkin_ml.optimizer import BelavkinOptimizer, AdaptiveBelavkinOptimizer\n",
    "from belavkin_ml.datasets.synthetic import ModularArithmeticDataset, create_dataloaders\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Simple Dataset\n",
    "\n",
    "We'll use modular addition: $f(x, y) = (x + y) \\mod p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modular arithmetic dataset\n",
    "p = 97  # Prime modulus\n",
    "dataset = ModularArithmeticDataset(\n",
    "    p=p,\n",
    "    operation='addition',\n",
    "    train_fraction=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get dataset info\n",
    "info = dataset.get_info()\n",
    "print(f\"Dataset Information:\")\n",
    "print(f\"  Modulus: {info['p']}\")\n",
    "print(f\"  Operation: {info['operation']}\")\n",
    "print(f\"  Training examples: {info['train_examples']}\")\n",
    "print(f\"  Test examples: {info['test_examples']}\")\n",
    "print(f\"  Input dimension: {info['input_dim']}\")\n",
    "print(f\"  Output dimension: {info['output_dim']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a Simple MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SimpleMLP(\n",
    "    input_dim=info['input_dim'],\n",
    "    output_dim=info['output_dim'],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Belavkin Optimizer\n",
    "\n",
    "Let's compare the Belavkin optimizer with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, optimizer, criterion, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, optimizer, criterion, train_loader, test_loader, n_epochs, device):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader, device)\n",
    "        test_loss, test_acc = evaluate(model, criterion, test_loader, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accs': test_accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train with Belavkin Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model_belavkin = SimpleMLP(\n",
    "    input_dim=info['input_dim'],\n",
    "    output_dim=info['output_dim'],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "# Create Belavkin optimizer\n",
    "optimizer_belavkin = BelavkinOptimizer(\n",
    "    model_belavkin.parameters(),\n",
    "    lr=1e-3,\n",
    "    gamma=1e-4,\n",
    "    beta=1e-2,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training with Belavkin Optimizer...\")\n",
    "results_belavkin = train_model(\n",
    "    model_belavkin,\n",
    "    optimizer_belavkin,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    n_epochs=50,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with Adam (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model_adam = SimpleMLP(\n",
    "    input_dim=info['input_dim'],\n",
    "    output_dim=info['output_dim'],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer_adam = torch.optim.Adam(\n",
    "    model_adam.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "print(\"Training with Adam Optimizer...\")\n",
    "results_adam = train_model(\n",
    "    model_adam,\n",
    "    optimizer_adam,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    n_epochs=50,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Test accuracy\n",
    "axes[0].plot(results_belavkin['test_accs'], label='Belavkin', linewidth=2)\n",
    "axes[0].plot(results_adam['test_accs'], label='Adam', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Test Accuracy Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test loss\n",
    "axes[1].plot(results_belavkin['test_losses'], label='Belavkin', linewidth=2)\n",
    "axes[1].plot(results_adam['test_losses'], label='Adam', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Loss')\n",
    "axes[1].set_title('Test Loss Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Belavkin - Best Test Acc: {max(results_belavkin['test_accs']):.4f}\")\n",
    "print(f\"Adam     - Best Test Acc: {max(results_adam['test_accs']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try Adaptive Belavkin Optimizer\n",
    "\n",
    "The adaptive variant automatically tunes hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model_adaptive = SimpleMLP(\n",
    "    input_dim=info['input_dim'],\n",
    "    output_dim=info['output_dim'],\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "# Create Adaptive Belavkin optimizer\n",
    "optimizer_adaptive = AdaptiveBelavkinOptimizer(\n",
    "    model_adaptive.parameters(),\n",
    "    lr=1e-3,\n",
    "    gamma=1e-4,\n",
    "    beta=1e-2,\n",
    "    adapt_lr=True,\n",
    "    adapt_gamma=True,\n",
    "    adapt_beta=True,\n",
    ")\n",
    "\n",
    "print(\"Training with Adaptive Belavkin Optimizer...\")\n",
    "results_adaptive = train_model(\n",
    "    model_adaptive,\n",
    "    optimizer_adaptive,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    n_epochs=50,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_belavkin['test_accs'], label='Belavkin', linewidth=2)\n",
    "plt.plot(results_adam['test_accs'], label='Adam', linewidth=2)\n",
    "plt.plot(results_adaptive['test_accs'], label='Adaptive Belavkin', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Optimizer Comparison on Modular Addition')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Comparison:\")\n",
    "print(f\"Belavkin         - Best: {max(results_belavkin['test_accs']):.4f}\")\n",
    "print(f\"Adam             - Best: {max(results_adam['test_accs']):.4f}\")\n",
    "print(f\"Adaptive Belavkin - Best: {max(results_adaptive['test_accs']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Adaptive Parameters\n",
    "\n",
    "The adaptive optimizer tracks how hyperparameters change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get adaptation statistics\n",
    "stats = optimizer_adaptive.get_adaptation_stats()\n",
    "\n",
    "print(\"Adaptation Statistics:\")\n",
    "print(f\"\\nCurrent Learning Rate: {stats['current_params']['group_0']['lr']:.6f}\")\n",
    "print(f\"Current Gamma: {stats['current_params']['group_0']['gamma']:.6f}\")\n",
    "print(f\"Current Beta: {stats['current_params']['group_0']['beta']:.6f}\")\n",
    "\n",
    "# Plot gradient norm history\n",
    "if stats['grad_norm_history']:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(stats['grad_norm_history'])\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Norm During Training')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run Full Benchmarks**: Use `experiments/track1_optimizer/run_modular_arithmetic.py`\n",
    "2. **Try Different Tasks**: Sparse parity, modular composition\n",
    "3. **Hyperparameter Tuning**: Search over gamma, beta ranges\n",
    "4. **Analyze Results**: Use visualization tools in `belavkin_ml.utils`\n",
    "5. **Scale Up**: Try on MNIST, CIFAR-10\n",
    "\n",
    "See the main README for more details!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
