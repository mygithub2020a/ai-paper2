% Bibliography for Belavkin Optimizer paper

@article{belavkin1992quantum,
  title={Quantum stochastic calculus and quantum nonlinear filtering},
  author={Belavkin, Viacheslav P},
  journal={Journal of Multivariate Analysis},
  volume={42},
  number={2},
  pages={171--201},
  year={1992}
}

@article{belavkin2005general,
  title={On the general form of quantum stochastic evolution equation},
  author={Belavkin, Viacheslav P},
  journal={arXiv preprint math/0512510},
  year={2005}
}

@book{belavkin2008quantum,
  title={Quantum Stochastics and Information: Statistics, Filtering and Control},
  author={Belavkin, Viacheslav P and Guta, Madalin},
  year={2008},
  publisher={World Scientific}
}

@inproceedings{welling2011sgld,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee Whye},
  booktitle={ICML},
  year={2011}
}

@article{raginsky2017non,
  title={Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal={COLT},
  year={2017}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{tieleman2012rmsprop,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{martens2020new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--76},
  year={2020}
}
